{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Llama 3.2 Vision on Amazon SageMaker\n",
    "\n",
    "[Llama 3.2](https://huggingface.co/blog/llama32) is the latest release of open LLMs from the Llama family released by Meta (as of October 2024); Llama 3.2 Vision comes in two sizes: 11B for efficient deployment and development on consumer-size GPU, and 90B for large-scale applications. \n",
    "\n",
    "\n",
    "\n",
    "In this blog you will learn how to deploy [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) to Amazon SageMaker. We are going to use the Hugging Face LLM DLC is a purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) a scalelable, optimized solution for deploying and serving Large Language Models (LLMs). The Blog post also includes Hardware requirements for the different model sizes. \n",
    "\n",
    "> [!WARNING]\n",
    "> Regarding the licensing terms, Llama 3.2 comes with a very similar license to Llama 3.1, with one key difference in the acceptable use policy: any individual domiciled in, or a company with a principal place of business in, the European Union (EU) is not being granted the license rights to use multimodal models included in Llama 3.2. This restriction does not apply to end users of a product or service that incorporates any such multimodal models, so people can still build global products with the vision variants.\n",
    ">\n",
    "> For full details, please make sure to read [the official license](https://huggingface.co/meta-llama/Llama-3.2-1B/blob/main/LICENSE.txt) and [the acceptable use policy](https://huggingface.co/meta-llama/Llama-3.2-1B/blob/main/USE_POLICY.md).\n",
    "\n",
    "In the blog will cover how to:\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Retrieve the new Hugging Face LLM DLC](#2-retrieve-the-new-hugging-face-llm-dlc)\n",
    "3. [Hardware requirements](#3-hardware-requirements)\n",
    "4. [Deploy Llama 3.2 11B to Amazon SageMaker](#4-deploy-llama-32-11b-to-amazon-sagemaker)\n",
    "5. [Run inference and chat with the model](#5-run-inference-and-chat-with-the-model)\n",
    "6. [Clean up](#5-clean-up)\n",
    "\n",
    "Lets get started!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy Idefics to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update version when image included \n",
    "!pip install \"sagemaker>=2.232.2\" --upgrade --quiet\n",
    "\n",
    "# install huggingface hub\n",
    "!pip install huggingface_hub --quiet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve the new Hugging Face LLM DLC\n",
    "\n",
    "Compared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our `HuggingFaceModel` model class with a `image_uri` pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the `get_huggingface_llm_image_uri` method provided by the `sagemaker` SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified `backend`, `session`, `region`, and `version`. You can find the available versions [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COMMENT IN WHEN IMAGE IS RELEASED\n",
    "# from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# # retrieve the llm image uri\n",
    "# llm_image = get_huggingface_llm_image_uri(\n",
    "#   \"huggingface\",\n",
    "#   version=\"2.3.1\"\n",
    "# )\n",
    "\n",
    "# # print ecr image uri\n",
    "# print(f\"llm image uri: {llm_image}\")\n",
    "\n",
    "llm_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/huggingface-pytorch-tgi-inference:2.4-tgi2.3-gpu-py311-cu124-ubuntu22.04\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hardware requirements\n",
    "\n",
    "Llama 3.2 comes in 2 different sizes - 11B & 90B parameters. The hardware requirements will vary based on the model size deployed to SageMaker. Below is a set up minimum requirements for each model size we tested. \n",
    "\n",
    "| Model        | Instance Type     |  # of GPUs per replica | \n",
    "|--------------|-------------------|-----------------------|\n",
    "| [Llama 3.2 11B](meta-llama/Llama-3.2-11B-Vision-Instruct) | `(ml.)g5/6.12xlarge` | 4 | \n",
    "| [Llama 3.2 90B](meta-llama/Llama-3.2-90B-Vision-Instruct) | `(ml.)g6e.48xlarge` | 8 | \n",
    "| [Llama 3.2 90B](meta-llama/Llama-3.2-90B-Vision-Instruct) | `(ml.)p4d.24xlarge`   | 8                     | \n",
    "\n",
    "\n",
    "\n",
    "_Note: Amazon SageMaker currently doesn't support instance slicing meaning, e.g. for Llama 3.2 90B you cannot run multiple replica on a single instance._\n",
    "\n",
    "These are the setups we have validated for Llama 3.2 11B and 90B models to work on SageMaker.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy Llama 3.2 11B to Amazon SageMaker\n",
    "\n",
    "To deploy [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `g6.12xlarge` instance type, which has 4 NVIDIA L4 GPUs and 96GB of GPU memory. \n",
    "\n",
    "As [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) is a gated model with restricted access on the European Union (EU), meaning that you need to accept the license agreement.\n",
    "\n",
    "To generate a token for the Hugging Face Hub, you can follow the instructions in [Hugging Face Hub - User access tokens](https://huggingface.co/docs/hub/en/security-tokens); the generated token can either be fine-grained to have access to the model, or just overall read-only access to your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we are logged in we can create our HuggingFaceModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from huggingface_hub import get_token\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g6.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"meta-llama/Llama-3.2-11B-Vision-Instruct\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(6000),  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(8192),  # Max length of the generation (including input text)\n",
    "  'HF_HUB_ENABLE_HF_TRANSFER': \"1\", # Enable HF transfer for faster downloads\n",
    "  'HUGGING_FACE_HUB_TOKEN': get_token(), # Hugging Face token\n",
    "  'MESSAGES_API_ENABLED': \"true\", # Enable messages API\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.g6.12xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run inference and chat with the model\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. We will use the `predict` method from the `predictor` to run inference on our endpoint. We deployed Llama 3.2 Vision with `MESSAGES_API_ENABLED=true` which allows us to use the OpenAI compatible messages API. This allows us to include \"type\" \"image_url\", which can be a link to an image or a base64 encoded image. To keep things realistic we are going to upload an image to s3 and create a pre-signed url to it, which we will use in our inference request. Thats how you could handle images in a real-world application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import Config\n",
    "\n",
    "s3 = sess.boto_session.client('s3', config=Config(signature_version='s3v4'))\n",
    "\n",
    "def upload_image(image_path):\n",
    "    # params\n",
    "    bucket = sess.default_bucket()\n",
    "    key = os.path.join(\"input\", os.path.basename(image_path))\n",
    "    \n",
    "    # Upload image to S3    \n",
    "    s3.upload_file(image_path, bucket, key)\n",
    "\n",
    "    # Generate pre-signed URL valid for 5 minutes\n",
    "    url = s3.generate_presigned_url(\n",
    "        ClientMethod='get_object', \n",
    "        Params={'Bucket': bucket, 'Key': key},\n",
    "        ExpiresIn=300\n",
    "    )\n",
    "    return url\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"How long does it take from invoice date to due date? Be short and concise.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": upload_image(\"../assets/invoice.png\")\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Make calls the endpoint including the prompt and parameters\n",
    "chat = llm.predict({\n",
    "  \"messages\":messages,\n",
    "  \"max_tokens\": 512,\n",
    "#   \"do_sample\": True,\n",
    "  \"top_p\": 0.95,\n",
    "  \"temperature\": 1.0,\n",
    "  \"stream\": False,\n",
    "})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"https://huggingface.co/datasets/huggingface/release-assets/resolve/main/invoice.png\" alt=\"Invoice Image\" /></td>\n",
    "        <td>\n",
    "            To calculate the time difference between the invoice date and the due date, we need to subtract the invoice date from the due date.<br><br>\n",
    "            Invoice Date: 11/02/2019<br>\n",
    "            Due Date: 26/02/2019<br><br>\n",
    "            Time Difference = Due Date - Invoice Date<br>\n",
    "            Time Difference = 26/02/2019 - 11/02/2019<br>\n",
    "            Time Difference = 15 days<br><br>\n",
    "            Therefore, it takes <strong>15 days</strong> from the invoice date to the due date.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
